{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for single channel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "from tifffile import imread\n",
    "from PIL import Image\n",
    "import PIL.ImageOps  \n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import csv\n",
    "import shutil\n",
    "from shutil import copy,copytree\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holder variables, change as appropriate \n",
    "protein_name='Dystrophin'\n",
    "split_image_height= 512\n",
    "split_image_width= 512\n",
    "subject_type= 'controls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefixing each file name with subject_IDs- DO NOT Run this if files names are already changed\n",
    "path= './Dataset'\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if not files:\n",
    "        continue\n",
    "    prefix = os.path.basename(root)\n",
    "    for f in files:\n",
    "        os.rename(os.path.join(root, f), os.path.join(root, \"{}_{}\".format(prefix, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting only tiff images with same folder structure\n",
    "shutil.copytree('./Dataset', './Dataset_TIFF' , ignore=shutil.ignore_patterns('*.jpg', '*.db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seperate_Folder_per_protein(src, dst, file_ending_with):\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        for file in files:\n",
    "            if file.endswith(file_ending_with):\n",
    "                path_file = os.path.join(root,file)\n",
    "                path_dst = os.path.join(dst,file)\n",
    "                copy(path_file,dst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src= f'./Dataset_TIFF/{subject_type}'\n",
    "dst= f'./Dataset_TIFF_Dystrophin/{subject_type}/'\n",
    "file_ending_with= f\"{protein_name}.ome.tiff\"\n",
    "Seperate_Folder_per_protein(src, dst, file_ending_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_create(path):\n",
    "    if (os.path.exists(path)) and (os.listdir(path) != []):\n",
    "        shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def crop(input_file, height, width):\n",
    "    img = Image.open(input_file)\n",
    "    img_width, img_height = img.size\n",
    "    for i in range(img_height//height):\n",
    "        for j in range(img_width//width):\n",
    "            box = (j*width, i*height, (j+1)*width, (i+1)*height)\n",
    "            yield img.crop(box)\n",
    "\n",
    "def split(inp_img_dir,out_dir, height, width, \n",
    "          start_num):\n",
    "    #image_dir = os.path.join(out_dir, 'images')\n",
    "    dir_create(out_dir)\n",
    "    #dir_create(image_dir)\n",
    "    \n",
    "    img_list = [f for f in\n",
    "                os.listdir(inp_img_dir)\n",
    "                if os.path.isfile(os.path.join(inp_img_dir, f))]\n",
    "    file_num = 0\n",
    "    for infile in img_list:\n",
    "        infile_path = os.path.join(inp_img_dir, infile)        \n",
    "        for k, piece in enumerate(crop(infile_path,\n",
    "                                       height, width), start_num):\n",
    "            img = Image.new('I;16', (height, width), 65535)\n",
    "            img.paste(piece)\n",
    "            img_path = os.path.join(out_dir, \n",
    "                                    infile.split('_')[0]+ '_UqCRC2'  #change\n",
    "                                    + str(k).zfill(5) + '.tiff')\n",
    "            img.save(img_path)\n",
    "       \n",
    "        file_num += 1\n",
    "        sys.stdout.write(\"\\rFile %s was processed.\" % file_num)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_img_dir = f'./Dataset_TIFF_{protein_name}/{subject_type}'  \n",
    "out_dir = f'./Dataset_TIFF_{protein_name}/{subject_type}'   \n",
    "height = split_image_height\n",
    "width = split_image_width\n",
    "start_num = 1\n",
    "\n",
    "split(inp_img_dir, out_dir, height, width, start_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Multichannel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tifffile import imread,imwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to merge all channels\n",
    "def channel_merger(img_list): # img_list need to be list of images of all protein expression images of a subject \n",
    "    \n",
    "    multi_channel_img = np.zeros((*img_list[0].shape,len(img_list)), np.uint8)\n",
    "    \n",
    "    for i, img in enumerate(img_list):\n",
    "        multi_channel_img[:,:,i]= img\n",
    "        \n",
    "    return multi_channel_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_controls = dir_create('./TIFF_Images_Concat/controls/') \n",
    "new_path_patients = dir_create('./TIFF_Images_Concat/patients/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_dirs= [ f.path for f in os.scandir('./Dataset_TIFF/controls/') if f.is_dir() ]\n",
    "patients_dirs= [ f.path for f in os.scandir('./Dataset_TIFF/patients/') if f.is_dir() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in controls_dirs:\n",
    "    X=sorted(glob(path+'/*.tiff'))\n",
    "    X = list(map(imread,X))\n",
    "    multi_channel_img= channel_merger(X)\n",
    "    np.save(str(new_path_controls) + path[-2:]+'_combined.npy' ,multi_channel_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in patients_dirs:\n",
    "    X=sorted(glob(path+'/*.tiff'))\n",
    "    X = list(map(imread,X))\n",
    "    multi_channel_img= channel_merger(X)\n",
    "    np.save(str(new_path_patients) + path[-2:]+'_combined.npy' ,multi_channel_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the multichannel array into patches\n",
    "splited_control_dir= dir_create('./Concat_Split_Images/Controls) \n",
    "splited_patient_dir= dir_create('./Concat_Split_Images/Patients) \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#controls\n",
    "files = glob.glob(str(new_path_controls) + '/**/*.npy', recursive=True)\n",
    "for file in files:\n",
    "    img = np.load(file)\n",
    "    patches_img = patchify(img, (512,512,10), step=256)\n",
    "    for i in range(patches_img.shape[0]):\n",
    "        for j in range(patches_img.shape[1]):\n",
    "            single_patch_img = patches_img[i, j, 0, :, :, :]\n",
    "            np.save(str(splited_control_dir) + \"C0\"+str(files.index(file)+1)+'_image_' + '_'+ str(i)+str(j)+'.npy', single_patch_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patients\n",
    "files = glob.glob(str(new_path_patients) + '/**/*.npy', recursive=True)\n",
    "for file in files:\n",
    "    img = np.load(file)\n",
    "    patches_img = patchify(img, (512,512,10), step=256)\n",
    "    for i in range(patches_img.shape[0]):\n",
    "        for j in range(patches_img.shape[1]):\n",
    "            single_patch_img = patches_img[i, j, 0, :, :, :]\n",
    "            np.save(str(splited_patient_dir) + \"P0\"+str(files.index(file)+1)+'_image_' + '_'+ str(i)+str(j)+'.npy', single_patch_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(r'./Concat_Split_Images')\n",
    "\n",
    "folders = ['Controls', 'Patients']\n",
    "\n",
    "files = []\n",
    "\n",
    "for folder in folders:\n",
    "    for file in os.listdir(folder):\n",
    "        files.append([str('./Concat_Split_Images/')+folder+str('/')+file, folder])\n",
    "\n",
    "pd.DataFrame(files, columns=['files', 'target']).to_csv('files_and_targets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
